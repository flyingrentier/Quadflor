# experiments for final evaluation with increasing samples factors
# for single fold:
# python run.py -f mlpsoph -t -k file_paths.json --fixed_folds --folds=1 -v --batch_size=256 -e 200 --val-size=0.2 --patience=10 --tf-model-path=<path/to/temporary/folder> --max_features=25000 --num_steps_before_validation=2000 --learning_rate=0.001 --dropout=0.5 --memory=0.3 -C ../../Experiments/final_mlp_experiments.cfg -o final_mlp.csv --embedding_size=0 --ngram_limit=2 --optimize_threshold 
# for cross-validation:
# python run.py -f mlpsoph -t -k file_paths_ks.json --fixed_folds --folds=10 -v --batch_size=256 -e 200 --val-size=0.2 --patience=10 --tf-model-path=<path/to/temporary/folder> --max_features=25000 --num_steps_before_validation=2000 --learning_rate=0.001 --dropout=0.5 --memory=0.3 -C ../../Experiments/final_mlp_experiments.cfg -o final_mlp.csv --embedding_size=0 --ngram_limit=2 --optimize_threshold
# EconBiz
--extra_samples_factor=1 -K econbiz --hidden_layers 2000
--extra_samples_factor=2 -K econbiz --hidden_layers 2000
--extra_samples_factor=4 -K econbiz --hidden_layers 2000
--extra_samples_factor=8 -K econbiz --hidden_layers 2000
--extra_samples_factor=100 -K econbiz --hidden_layers 2000
# PubMed
--extra_samples_factor=1 -K pubmed --hidden_layers 1000 1000 --batch_norm --dropout=1.
--extra_samples_factor=2 -K pubmed --hidden_layers 1000 1000 --batch_norm --dropout=1.
--extra_samples_factor=4 -K pubmed --hidden_layers 1000 1000 --batch_norm --dropout=1.
--extra_samples_factor=8 -K pubmed --hidden_layers 1000 1000 --batch_norm --dropout=1.
--extra_samples_factor=100 -K pubmed --hidden_layers 1000 1000 --batch_norm --dropout=1.
